# Infrastructure Specialist Checkpoint

**Date**: 2026-02-12
**Task**: Extract Kubernetes observability configs from setup.sh to manifest files and update guard to validate dashboard-Loki label consistency

---

## Patterns Discovered

### 1. Multi-Document YAML Files for Kubernetes Observability

Each observability component (Promtail, Loki, Prometheus) naturally groups multiple resources:
- ConfigMap (configuration)
- ServiceAccount, ClusterRole, ClusterRoleBinding (RBAC)
- Deployment/StatefulSet/DaemonSet (workload)
- Service (networking)

Using `---` separators in a single file keeps related resources together, making it easier to understand the full deployment of each component.

### 2. Kustomization for Observability Stack

Created `kustomization.yaml` to enable:
- Single-command deployment: `kubectl apply -k infra/kubernetes/observability/`
- Common labels for all resources (`managed-by: dark-tower`, `environment: dev`)
- Foundation for environment-specific overlays (dev/staging/prod)

### 3. Guard-Driven Dashboard Validation

The guard now validates dashboard queries against Promtail's relabel_configs, ensuring:
- Dashboard Loki labels match what Promtail actually exports
- `job` label (common in Prometheus examples) isn't used when Promtail exports `app` instead
- Changes to Promtail config will fail guards if dashboards aren't updated

---

## Gotchas Encountered

### 1. Promtail Exports `app` Label, Not `job`

The existing dashboards used `job="service-name"` pattern (common in Prometheus world), but Promtail's relabel_configs export:
- `namespace` (from `__meta_kubernetes_namespace`)
- `pod` (from `__meta_kubernetes_pod_name`)
- `container` (from `__meta_kubernetes_pod_container_name`)
- `app` (from `__meta_kubernetes_pod_label_app`)
- `component` (from `__meta_kubernetes_pod_label_component`)

**No `job` label exists.** Dashboards must use `app=` instead of `job=`.

### 2. Python YAML Parsing with Inline Heredocs

When passing file paths to inline Python scripts in bash, use variable interpolation:
```bash
python3 -c "
config_file = '$PROMTAIL_CONFIG'
with open(config_file, 'r') as f:
    ...
"
```

Not command-line arguments to heredocs:
```bash
# This fails - sys.argv[1] doesn't exist
python3 << 'EOF'
import sys
with open(sys.argv[1], 'r') as f:
EOF
"$PROMTAIL_CONFIG"  # This won't be passed to the script
```

### 3. Multi-Document YAML in Kubernetes ConfigMaps

The Promtail ConfigMap contains embedded YAML (promtail.yaml) that needs to be parsed separately:
1. First parse the Kubernetes manifest documents
2. Find the ConfigMap with `data.promtail.yaml`
3. Parse that embedded YAML to get the actual Promtail config

### 4. Dashboard Variable Query Mismatch (Iteration 2)

Dashboard template variables must query the correct Loki label. Found variables named `pod` that were querying the `container` label instead. This caused "Recent Logs" panels to show no data even though volume charts worked.

**Symptom**: Variable dropdown populated with container names, but queries filtered by `pod=~"$pod"` which didn't match.

**Solution**: Update variable `query.label` field to match the variable name.

---

## Key Decisions

### 1. Kept Grafana Dashboard/Datasource Provisioning Separate

The Grafana deployment in setup.sh uses `kubectl create configmap --from-file=` for dashboards. This was intentionally kept as-is because:
- Dashboards are in `infra/grafana/dashboards/*.json` (already separate files)
- Datasources are in `infra/grafana/provisioning/datasources/datasources.yaml` (already separate)
- Only the Grafana Deployment itself remains inline (simple, rarely changes)

### 2. Used Python for YAML Parsing Instead of `yq`

`yq` wasn't available in the environment, but Python with PyYAML was. This choice:
- Avoids adding new tool dependencies
- Works reliably across environments
- Handles multi-document YAML and nested configs well

### 3. Special-Cased `level` Label in Guard

The guard allows `level` in Loki queries even though it's not in relabel_configs because:
- `level` is extracted by Promtail's pipeline stages (regex + labels promotion)
- It's a standard pattern for filtering by log severity
- Extracted from log message content using regex, not from Kubernetes metadata
- Adding it to relabel_configs would be redundant (it's not a K8s label)

### 4. Added Monitoring Stack for Container Metrics (Iteration 2)

Created kube-state-metrics and node-exporter deployments to provide infrastructure metrics:
- **kube-state-metrics**: Cluster-level metrics (pod states, deployments, replica counts)
- **node-exporter**: Node-level metrics (CPU, memory, disk, network)
- **kubelet cAdvisor**: Container-level metrics via Prometheus scrape config

This enables overview dashboard infrastructure panels to show CPU/memory usage.

---

## Current Status

**Implementation Complete - Iteration 4**

All tasks completed (including code review fixes):
1. Extracted Promtail, Loki, Prometheus configs to `infra/kubernetes/observability/`
2. Created kube-state-metrics and node-exporter manifests (Iteration 2)
3. Updated Prometheus config with scrape jobs for kubelet, kube-state-metrics, node-exporter (Iteration 2)
4. Updated setup.sh to deploy all observability components
5. Enhanced grafana-datasources guard with Loki label validation (Check 5)
6. Enhanced grafana-datasources guard with variable consistency check (Check 6, Iteration 2)
7. Enhanced grafana-datasources guard with Prometheus query validation (Check 7, Iteration 3)
8. Added log level extraction to Promtail pipeline (Iteration 4)
9. Fixed dashboard files to use correct labels (`app` instead of `job`)
10. Fixed dashboard variable queries to match variable names (Iteration 2)

All 7 verification layers pass.

---

## Code Review Iteration 2 - Fixes Applied

### Fix 1: Dashboard Variable Label Mismatch (BLOCKER)
Fixed template variables in log dashboards that were querying `container` label but named `pod`. Updated all three log dashboards to query the correct `pod` label.

### Fix 2: Monitoring Stack Deployment (BLOCKER)
Created kube-state-metrics and node-exporter manifests, added scrape configs to Prometheus, and updated setup.sh to deploy them. This enables CPU/memory metrics in overview dashboards.

### Fix 3: Variable Consistency Guard (ENHANCEMENT)
Added Check 6 to grafana-datasources guard to validate:
- Variable names match queried labels
- Variables query valid Loki labels from Promtail config

This prevents future variable/label mismatches.

### Fix 4: Prometheus Query Validation Guard (ENHANCEMENT - Iteration 3)
Added Check 7 to grafana-datasources guard to validate:
- Detects Docker patterns in Prometheus queries (`name=~"..."`)
- Validates infrastructure labels (namespace, pod, node, etc.)
- Dynamically extracts valid labels from Prometheus config
- Currently informational only (not blocking) - detects issues for future fixes

**What it detects:**
- Docker label patterns (invalid in Kubernetes): `{name=~"dark_tower_.*"}`
- Suggests Kubernetes equivalents: `{namespace="dark-tower", pod=~"ac-service.*"}`
- Validates infrastructure labels against Prometheus scrape configs
- Ignores application-specific metric labels (status, error_type, etc.)

**Implementation approach:**
- Parses Prometheus ConfigMap dynamically (no hardcoded labels)
- Extracts standard Kubernetes labels from `kubernetes_sd_configs`
- Extracts custom labels from `relabel_configs`
- Only validates infrastructure labels, not application metric labels

### Fix 5: Log Level Label Extraction (BLOCKER - Iteration 4)
Added log level extraction to Promtail pipeline configuration to enable log filtering by severity.

**Problem:** Log detail panels in dashboards showed no data when filtering by level (error, warn, etc.) because `level` was not available as a Loki label.

**Solution:** Updated Promtail pipeline_stages to:
1. Parse CRI format (already present)
2. Extract log level from message using regex: `^\[.*?\]\s+(?:\x1b\[[\d;]+m)?\s*(?P<level>TRACE|DEBUG|INFO|WARN|ERROR)\s+`
3. Promote extracted `level` field to a Loki label

**Benefits:**
- Log level is now indexed (faster queries)
- Dashboard queries work: `{app="ac-service"} | level="error"`
- Low cardinality (only 5 values: TRACE/DEBUG/INFO/WARN/ERROR)
- Standard Grafana log level filtering compatibility

**Files modified:**
- `infra/kubernetes/observability/promtail-config.yaml`: Added regex and labels stages to pipeline

### Iteration 5: Switch to JSON Structured Logging (BLOCKER Fix)

**Issue:** Regex parsing of log levels from formatted text logs is brittle and error-prone. Any changes to log formatting (ANSI codes, timestamp format) will break parsing.

**Root Cause:** Services emitted human-readable text logs with ANSI color codes requiring complex regex patterns. This approach is:
- Brittle: Breaks when log format changes
- Hard to maintain: Complex regex debugging
- Incomplete: Can't reliably extract structured metadata
- Production anti-pattern: Text parsing is not robust

**Solution:** Switch all services to JSON structured logging:

1. **Updated all service main.rs files** to emit JSON logs:
   - `crates/ac-service/src/main.rs`
   - `crates/global-controller/src/main.rs`
   - `crates/meeting-controller/src/main.rs`
   - `crates/media-handler/src/main.rs`

   Changed from: `tracing_subscriber::fmt::layer()`
   To: `tracing_subscriber::fmt::layer().json()`

2. **Updated Promtail configuration** to parse JSON logs:
   - Replaced brittle regex stage with robust json stage
   - Extracts: level, target, timestamp from JSON fields
   - Promotes level and target to Loki labels

3. **Updated guard validation** to extract labels from pipeline_stages:
   - Added logic to parse labels from pipeline_stages (json expressions + labels)
   - Guard now validates both relabel_configs AND pipeline_stages

**Benefits:**
- **Robust parsing:** JSON is standard format, no regex fragility
- **Richer metadata:** Can extract target, span_id, request_id as labels
- **Industry standard:** JSON logs are best practice for containerized apps
- **Future-proof:** Adding fields doesn't break parsing
- **Better queries:** Filter by any JSON field, not just level

**Files modified:**
- `crates/ac-service/src/main.rs`: Added .json() to tracing subscriber
- `crates/global-controller/src/main.rs`: Added .json() to tracing subscriber
- `crates/meeting-controller/src/main.rs`: Added .json() to tracing subscriber
- `crates/media-handler/src/main.rs`: Added tracing subscriber with .json()
- `infra/kubernetes/observability/promtail-config.yaml`: Replaced regex with json stage
- `scripts/guards/simple/grafana-datasources.sh`: Added pipeline_stages label extraction

### Iteration 6: Fix Log Dashboard Variable and Query Issues (BLOCKER Fix)

**Issue:** After implementing JSON structured logging, log dashboards still don't work due to three issues:
1. Variable name mismatch (`log_level` vs `level`)
2. Hard-coded lowercase values don't match uppercase JSON logs
3. Incorrect query syntax (line filter instead of label selector)

**Root Cause:**
- Dashboard variables named `log_level` with hard-coded lowercase values (`"error"`, `"warn"`, etc.)
- Actual JSON logs emit uppercase levels (`"ERROR"`, `"WARN"`, etc.)
- Queries used line filter syntax `|~ "${log_level}"` instead of label selector syntax
- `level` is now an indexed Loki label (not text content), requires label selector syntax

**Solution:**

1. **Renamed and made variable dynamic:**
   - Changed from `"name": "log_level"` to `"name": "level"`
   - Changed from `"type": "custom"` to `"type": "query"`
   - Queries Loki dynamically: `{"label": "level", "stream": ""}`
   - Auto-discovers available log levels (ERROR, WARN, INFO, DEBUG, TRACE)
   - Includes "All" option via `includeAll: true`

2. **Fixed query syntax:**
   - Before: `{app="ac-service", pod=~"$pod"} |~ "${log_level}"` (WRONG - line filter)
   - After: `{app="ac-service", pod=~"$pod", level=~"$level"}` (CORRECT - label selector)
   - Both `pod` and `level` are indexed labels, use same selector syntax
   - Fixed hard-coded queries: `| level="error"` â†’ `, level="ERROR"` (uppercase)

**Benefits:**
- Dynamic variable auto-discovers log levels from actual data
- No hard-coding - resilient to changes
- Correct case matching (uppercase)
- Consistent label selector syntax for all indexed labels
- All four panels now show data (Log Volume, Recent Logs, Error Logs, Warning Logs)

**Files modified:**
- `infra/grafana/dashboards/ac-logs.json`: Variable + 4 panel queries
- `infra/grafana/dashboards/gc-logs.json`: Variable + 4 panel queries
- `infra/grafana/dashboards/mc-logs.json`: Variable + 4 panel queries

---

## Files Created

- `infra/kubernetes/observability/promtail-config.yaml`
- `infra/kubernetes/observability/loki-config.yaml`
- `infra/kubernetes/observability/prometheus-config.yaml`
- `infra/kubernetes/observability/kube-state-metrics.yaml` (Iteration 2)
- `infra/kubernetes/observability/node-exporter.yaml` (Iteration 2)
- `infra/kubernetes/observability/kustomization.yaml`

## Files Modified

- `infra/kind/scripts/setup.sh` (Iterations 1 & 2)
- `scripts/guards/simple/grafana-datasources.sh` (Iterations 1, 2, 3, 4 & 5)
- `infra/grafana/dashboards/ac-logs.json` (Iterations 1, 2 & 6)
- `infra/grafana/dashboards/gc-logs.json` (Iterations 1, 2 & 6)
- `infra/grafana/dashboards/mc-logs.json` (Iterations 1, 2 & 6)
- `infra/grafana/dashboards/errors-overview.json` (Iteration 1)
- `infra/kubernetes/observability/prometheus-config.yaml` (Iteration 2)
- `infra/kubernetes/observability/promtail-config.yaml` (Iterations 4 & 5)
- `crates/ac-service/src/main.rs` (Iteration 5)
- `crates/global-controller/src/main.rs` (Iteration 5)
- `crates/meeting-controller/src/main.rs` (Iteration 5)
- `crates/media-handler/src/main.rs` (Iteration 5)

## Known Issues Detected (Informational)

**Check 7 Findings (Not Blocking):**
- `ac-overview.json` uses Docker `name` label pattern in Prometheus queries
- These queries will not work in Kubernetes environment
- Kubernetes replacement needed: `{namespace="dark-tower", pod=~"ac-service.*"}`
- Currently informational only - will be enforced in future iterations

---

## Reflection Summary

**Task**: Extract Kubernetes observability configs from inline YAML to manifest files, implement schema validation guards, switch to JSON structured logging, and fix dashboard queries.

**Key Learnings**:

1. **Dynamic config parsing prevents drift**: Guards that parse source configs (Promtail, Prometheus) to extract valid values stay synchronized automatically. Hardcoded label lists become stale.

2. **JSON structured logging requires query syntax changes**: When log fields are extracted to indexed Loki labels, dashboard queries must use label selector syntax `{level="ERROR"}`, not line filter syntax `|~ "ERROR"`. This is a fundamental semantic difference.

3. **Dynamic variables are more resilient**: Query-type Grafana variables auto-discover values from data sources, preventing case mismatches and adapting to schema changes.

**Patterns documented**:
- Dynamic config parsing in bash guards (Python + YAML parsing)
- Multi-document observability manifests (single source of truth)

**Gotchas documented**:
- Label selector vs line filter syntax for indexed labels
- Hard-coded variable values must match log case (or use dynamic queries)
- Python heredocs don't accept command-line arguments (already existed)

**Knowledge files updated**:
- Added 1 pattern (dynamic config parsing)
- Added 2 gotchas (JSON log query syntax, variable case sensitivity)
- No pruning needed (existing entries remain relevant)
